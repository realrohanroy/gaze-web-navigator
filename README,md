Gaze-Based Web Navigator ğŸ‘ï¸ğŸ–±ï¸

A webcam-based eye-tracking system that enables hands-free web navigation using gaze input.
The system estimates eye gaze in real time, maps it to screen coordinates through user-specific calibration, and controls a browser cursor via WebSockets.

This project focuses on Humanâ€“Computer Interaction (HCI), accessibility, and real-time computer vision, using only a standard RGB webcam (no special hardware).

ğŸš€ Features

Real-time eye gaze estimation using webcam

Iris-based gaze direction detection (MediaPipe FaceMesh)

User-specific 9-point calibration

Resolution-independent gaze mapping (normalized coordinates)

Exponential smoothing to reduce jitter

WebSocket-based communication between Python backend and browser

Browser-based gaze cursor overlay

Modular, extensible architecture (ready for dwell-to-click, pause, recalibration)

ğŸ§  System Overview
High-level architecture
Webcam
  â†“
MediaPipe FaceMesh
  â†“
Iris & eye landmark extraction
  â†“
Normalized gaze vector computation
  â†“
Calibration (linear regression)
  â†“
Smoothed gaze coordinates
  â†“
WebSocket (Python â†’ Browser)
  â†“
Browser cursor rendering


The backend handles vision, math, and calibration, while the frontend handles UI and interaction logic.

ğŸ› ï¸ Tech Stack
Backend

Python 3.10

OpenCV

MediaPipe FaceMesh

NumPy

WebSockets (asyncio)

Frontend

HTML / CSS

Vanilla JavaScript

WebSocket API

Concepts Used

Computer Vision

Humanâ€“Computer Interaction (HCI)

Signal smoothing (Exponential Moving Average)

Linear regression

Real-time systems

Clientâ€“server communication

ğŸ“‚ Project Structure
gaze-web-navigator/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # Final gaze â†’ browser pipeline
â”‚   â”œâ”€â”€ calibration_debug.py    # Calibration logic (one-time)
â”‚   â”œâ”€â”€ websocket_server.py     # WebSocket utilities
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Browser UI
â”‚   â”œâ”€â”€ styles.css              # Cursor styling
â”‚   â””â”€â”€ gaze.js                 # WebSocket + cursor logic
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ calibration.json        # Saved user calibration
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ demo.mp4

âš™ï¸ How It Works (Detailed)
1. Eye & Iris Tracking

MediaPipe FaceMesh detects facial landmarks.

Iris centers and eye corner landmarks are extracted.

Iris displacement relative to eye center gives gaze direction.

2. Normalized Gaze Vector

Raw pixel offsets are normalized by eye width, making gaze estimation:

Independent of face size

Less sensitive to camera distance

3. Calibration (Critical Step)

A 9-point calibration is performed:

User looks at known screen positions

Corresponding gaze values are recorded

Linear regression learns a mapping:

screen_x = a * gaze_x + b
screen_y = c * gaze_y + d


Calibration data is saved to disk (calibration.json) and reused.

4. Smoothing

Raw gaze data is noisy due to:

Micro-saccades

Camera noise

Landmark jitter

Exponential smoothing is applied:

smoothed = Î± * new + (1 âˆ’ Î±) * previous


This significantly improves cursor stability while keeping latency low.

5. Browser Integration

Backend sends normalized gaze coordinates (0â€“1) over WebSocket.

Browser scales them to current viewport size.

Cursor works correctly in fullscreen, windowed, or resized states.

â–¶ï¸ Running the Project
1. Install dependencies
pip install opencv-python mediapipe numpy websockets

2. Run calibration (one time per setup)
python backend/calibration_debug.py


This generates:

data/calibration.json

3. Start backend
python backend/main.py

4. Start frontend
cd frontend
python -m http.server 5500

5. Open browser
http://localhost:5500

âš ï¸ Known Limitations (Explicit & Honest)

Requires relatively stable head position after calibration

Accuracy degrades with large head movement

Glasses and poor lighting can affect tracking quality

Webcam-based tracking is less precise than IR eye trackers

These limitations are expected for RGB webcam eye tracking and are clearly documented by design.

â™¿ Accessibility Use Case

This system can be extended for:

Hands-free browsing

Assistive technology for motor impairments

Eye-based UI navigation

Research in attention tracking and UX

ğŸ§ª Future Improvements

Dwell-to-click interaction

Pause / resume tracking hotkey

Head pose compensation

Dynamic re-calibration

Heatmaps for attention analysis

Multi-monitor support

ğŸ¥ Demo

A short demo video (demo.mp4) shows:

Calibration

Live gaze cursor

Browser interaction

ğŸ§¾ Resume Description (Ready to Use)

Gaze-Based Web Navigator
Built a real-time webcam-based eye-tracking system enabling hands-free web navigation using gaze input. Implemented iris-based gaze estimation with MediaPipe FaceMesh, user-specific calibration via linear regression, and jitter reduction using exponential smoothing. Integrated Python backend with browser UI using WebSockets to control a gaze-driven cursor in real time. Designed the system with accessibility and resolution independence in mind.